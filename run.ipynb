{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9901298522949219"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nvidia_smi\n",
    "\n",
    "def get_gpu_allocated_mem():\n",
    "    nvidia_smi.nvmlInit()\n",
    "    deviceCount = nvidia_smi.nvmlDeviceGetCount()\n",
    "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "    return info.free/info.total\n",
    "\n",
    "get_gpu_allocated_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "__file__ = Path(os.path.abspath(\".\")) / \"src\" / \"training.py\"\n",
    "dataset_name = \"biclfs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from typing import NamedTuple, OrderedDict\n",
    "import numpy as np\n",
    "from ict_2 import ICT\n",
    "from tqdm.notebook import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "parent_dir = Path(__file__).parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d6d7d6acdf42b7b6ddb575b1e7c143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Table Level Loop:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0068c450b34445cc8b93aa88139028a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fold Loop: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea228b7ad494b31a9d628d76f34fb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HP Level Loop:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44022004ec1480ba083c01d20335944",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing training examples.:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1190 > 1024). Running this sequence through the model will result in indexing errors\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1190. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1036. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1152. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1071. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1052. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1141. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1030. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1039. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1255. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1090. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1067. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1065. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1025. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1054. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1084. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1063. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1116. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1104. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1035. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1062. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1145. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1061. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1050. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1139. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1066. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1057. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1125. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1042. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1058. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1043. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1041. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1034. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1091. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1150. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1040. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1028. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1099. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1026. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1108. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1128. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1056. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n",
      "/home/mila/c/cesare.spinoso/In-context-Tuning/src/data_loader.py:175: UserWarning: MODEL LENGTH EXCEEDED. Length of input text is 1167. This exceeds the model's max length of 1024.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b840db214eb24cee97dbfd9cdebb418e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch training.:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850ea7a6252841ce876a2899bc116720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch training.:   0%|          | 0/301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mila/c/cesare.spinoso/.conda/envs/ict/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 173>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=169'>170</a>\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_dir \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtable_level_results_biclfs.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=170'>171</a>\u001b[0m             pkl\u001b[39m.\u001b[39mdump(table_level_results, f)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=172'>173</a>\u001b[0m main()\n",
      "\u001b[1;32m/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb Cell 5\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m ict \u001b[39m=\u001b[39m ICT(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=89'>90</a>\u001b[0m     model_name\u001b[39m=\u001b[39mmodel_name,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m     task_format\u001b[39m=\u001b[39mtask_format,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m     device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=92'>93</a>\u001b[0m     identifier\u001b[39m=\u001b[39mict_identifier,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=93'>94</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=94'>95</a>\u001b[0m \u001b[39m# Meta-train\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=95'>96</a>\u001b[0m ict\u001b[39m.\u001b[39;49mmeta_train(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m     task2template_examples\u001b[39m=\u001b[39;49mtrain_task2examples,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=97'>98</a>\u001b[0m     task2verbalizers\u001b[39m=\u001b[39;49mtrain_task2verbalizers,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m     num_demonstrations\u001b[39m=\u001b[39;49mnum_demonstrations,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m     example_delimiter\u001b[39m=\u001b[39;49mexample_delimiter,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m     allow_label_overlap\u001b[39m=\u001b[39;49mallow_label_overlap,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m     num_warmup_steps\u001b[39m=\u001b[39;49mnum_warmup_steps,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m     num_epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m     bsz\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m     output_dir\u001b[39m=\u001b[39;49mparent_dir \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39moutput_biclfs\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m/\u001b[39;49m ict_identifier,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=106'>107</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39m# Meta-test on val\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m _, val_task2scores \u001b[39m=\u001b[39m ict\u001b[39m.\u001b[39mmeta_test(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=109'>110</a>\u001b[0m     val_task2examples,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=110'>111</a>\u001b[0m     task2verbalizers\u001b[39m=\u001b[39mval_task2verbalizers,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m     bsz\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcn-g014.server.mila.quebec/home/mila/c/cesare.spinoso/In-context-Tuning/run.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m )\n",
      "File \u001b[0;32m~/In-context-Tuning/src/ict_2.py:172\u001b[0m, in \u001b[0;36mICT.meta_train\u001b[0;34m(self, task2template_examples, task2verbalizers, num_demonstrations, example_delimiter, allow_label_overlap, lr, num_warmup_steps, num_epochs, bsz, output_dir)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mamp\u001b[39m.\u001b[39mautocast():\n\u001b[1;32m    164\u001b[0m     \u001b[39m# Loss for k-way classification\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     loss, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mforward(\n\u001b[1;32m    166\u001b[0m         input_dict,\n\u001b[1;32m    167\u001b[0m         torch\u001b[39m.\u001b[39mLongTensor(data_loader\u001b[39m.\u001b[39mtask2verbalizer_worids[task])\u001b[39m.\u001b[39mto(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    170\u001b[0m         torch\u001b[39m.\u001b[39mLongTensor(labels)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice),\n\u001b[1;32m    171\u001b[0m     )\n\u001b[0;32m--> 172\u001b[0m scaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    173\u001b[0m scaler\u001b[39m.\u001b[39mstep(optimizer)\n\u001b[1;32m    174\u001b[0m scaler\u001b[39m.\u001b[39mupdate()\n",
      "File \u001b[0;32m~/.conda/envs/ict/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/ict/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class TableKey(NamedTuple):\n",
    "    model_name: str\n",
    "    num_demonstrations: int\n",
    "\n",
    "\n",
    "class HPKey(NamedTuple):\n",
    "    number_of_epochs: int\n",
    "    learning_rate: float\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Table setup\n",
    "    model_names = [\"gpt2\", \"gpt2-medium\", \"gpt2-large\"]\n",
    "    number_of_demonstrations = [5, 0]\n",
    "    task_format = \"clm\"\n",
    "    table_level_combinations = list(\n",
    "        itertools.product(model_names, number_of_demonstrations)\n",
    "    )\n",
    "    # Training hyper-parameters\n",
    "    number_of_epochs = [10, 15, 30]  # This might be too much\n",
    "    learning_rates = [1e-7, 3e-7, 1e-6, 3e-6]\n",
    "    example_delimiter = \" \"\n",
    "    batch_size = 8  # Smaller batch size than lama\n",
    "    num_warmup_steps = 100\n",
    "    # Set this to True otherwise a positive example could only\n",
    "    # learn from all negative examples\n",
    "    allow_label_overlap = True\n",
    "    device = \"cuda\"\n",
    "    num_prefix_selections = 20  # Large number of selections than lama\n",
    "    metrics = [\"mrr\", \"precision1\", \"precision10\"]\n",
    "    hp_level_combinations = list(itertools.product(number_of_epochs, learning_rates))\n",
    "\n",
    "    # Prepare data\n",
    "    cv_split = pkl.load(\n",
    "        open(parent_dir / \"data_biclfs\" / \"cross_validation_splits.pkl\", \"rb\")\n",
    "    )\n",
    "    training_data = pkl.load(\n",
    "        open(parent_dir / \"data_biclfs\" / \"training_data_templated.pkl\", \"rb\")\n",
    "    )\n",
    "    testing_data = pkl.load(\n",
    "        open(parent_dir / \"data_biclfs\" / \"testing_data_templated.pkl\", \"rb\")\n",
    "    )\n",
    "\n",
    "    # Load verbalizers\n",
    "    verbalizers = []\n",
    "    with open(parent_dir / \"data_biclfs\" / \"class_verbalizers.txt\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            word = line.strip()\n",
    "            if len(word) != 0:\n",
    "                verbalizers.append(word)\n",
    "\n",
    "    # Prepare data structures to hold results for the table\n",
    "    # Trying to replicate\n",
    "    table_level_results = {}\n",
    "    fold_level_results = {}\n",
    "    selected_model_names = {}\n",
    "\n",
    "    for model_name, num_demonstrations in tqdm(\n",
    "        table_level_combinations,\n",
    "        desc=f\"Table Level Loop\",\n",
    "    ):\n",
    "        table_level_results[(model_name, num_demonstrations)] = []\n",
    "        fold_level_results[(model_name, num_demonstrations)] = []\n",
    "        selected_model_names[(model_name, num_demonstrations)] = []\n",
    "        for fold_idx, fold in tqdm(enumerate(cv_split), desc=f\"Fold Loop\"):\n",
    "            # Each fold is like a model with a training, validation and testing set\n",
    "            # Find optimal HP based on validation set and then get the test set results\n",
    "            # Get the tasks for this fold\n",
    "            train_tasks, val_tasks, test_tasks = (\n",
    "                fold[\"train\"],\n",
    "                fold[\"val\"],\n",
    "                fold[\"test\"],\n",
    "            )\n",
    "            # Training\n",
    "            train_task2examples = {task: training_data[task] for task in train_tasks if len(training_data[task]) > num_demonstrations}\n",
    "            train_task2verbalizers = {task: verbalizers for task in train_tasks}\n",
    "            # Validation\n",
    "            val_task2examples = {task: testing_data[task] for task in val_tasks}\n",
    "            val_task2verbalizers = {task: verbalizers for task in val_tasks}\n",
    "            # Testing\n",
    "            test_task2examples = {task: testing_data[task] for task in test_tasks}\n",
    "            test_task2verbalizers = {task: verbalizers for task in test_tasks}\n",
    "\n",
    "            # Best val score tracker\n",
    "            best_val_score = 0\n",
    "            ict_max = None\n",
    "            for epochs, lr in tqdm(hp_level_combinations, desc=f\"HP Level Loop\"):\n",
    "                ict_identifier = f\"model_{model_name}_k_{num_demonstrations}_fold_{fold_idx}_epochs_{epochs}_lr_{lr}\"\n",
    "                ict = ICT(\n",
    "                    model_name=model_name,\n",
    "                    task_format=task_format,\n",
    "                    device=device,\n",
    "                    identifier=ict_identifier,\n",
    "                )\n",
    "                # Meta-train\n",
    "                ict.meta_train(\n",
    "                    task2template_examples=train_task2examples,\n",
    "                    task2verbalizers=train_task2verbalizers,\n",
    "                    num_demonstrations=num_demonstrations,\n",
    "                    example_delimiter=example_delimiter,\n",
    "                    allow_label_overlap=allow_label_overlap,\n",
    "                    lr=lr,\n",
    "                    num_warmup_steps=num_warmup_steps,\n",
    "                    num_epochs=epochs,\n",
    "                    bsz=batch_size,\n",
    "                    output_dir=parent_dir / \"output_biclfs\" / ict_identifier,\n",
    "                )\n",
    "                # Meta-test on val\n",
    "                _, val_task2scores = ict.meta_test(\n",
    "                    val_task2examples,\n",
    "                    task2verbalizers=val_task2verbalizers,\n",
    "                    num_demonstrations=num_demonstrations,\n",
    "                    example_delimiter=example_delimiter,\n",
    "                    allow_label_overlap=False,\n",
    "                    num_prefix_selections=num_prefix_selections,\n",
    "                    bsz=batch_size,\n",
    "                )\n",
    "                # Average across tasks\n",
    "                metric2scores = OrderedDict(\n",
    "                    {\n",
    "                        metric: [\n",
    "                            task_score[metric]\n",
    "                            for task_score in val_task2scores.values()\n",
    "                        ]\n",
    "                        for metric in metrics\n",
    "                    }\n",
    "                )\n",
    "                val_metric2avg_scores = [\n",
    "                    np.mean(scores) for scores in metric2scores.values()\n",
    "                ]\n",
    "                # Track best val score, only meta-test best ict\n",
    "                # Use precision1 as the metric to track\n",
    "                # Both for LAMA and BiCLFS (where p@1 is the accuracy)\n",
    "                if val_metric2avg_scores[1] > best_val_score:\n",
    "                    best_val_score = val_metric2avg_scores[1]\n",
    "                    ict_max = deepcopy(ict)\n",
    "            # Meta-test with ict_max on test\n",
    "            _, test_task2scores = ict_max.meta_test(\n",
    "                test_task2examples,\n",
    "                task2verbalizers=test_task2verbalizers,\n",
    "                num_demonstrations=num_demonstrations,\n",
    "                example_delimiter=example_delimiter,\n",
    "                allow_label_overlap=False,\n",
    "                num_prefix_selections=num_prefix_selections,\n",
    "                bsz=batch_size,\n",
    "            )\n",
    "            # Save the best models for each fold (to be used afterwards)\n",
    "            selected_model_names[(model_name, num_demonstrations)].append(\n",
    "                ict_max.identifier\n",
    "            )\n",
    "            # Average across tasks\n",
    "            test_metric = [\n",
    "                np.mean(\n",
    "                    [task_score[metric] for task_score in test_task2scores.values()]\n",
    "                )\n",
    "                for metric in metrics\n",
    "            ]\n",
    "            fold_level_results[(model_name, num_demonstrations)].append(test_metric)\n",
    "        # Average across folds\n",
    "        table_level_results[(model_name, num_demonstrations)] = np.mean(\n",
    "            fold_level_results[(model_name, num_demonstrations)], axis=0\n",
    "        )\n",
    "        # Pickle the table results as they come in\n",
    "        with open(parent_dir / \"results\" / \"fold_level_results_biclfs.pkl\", \"wb\") as f:\n",
    "            pkl.dump(fold_level_results, f)\n",
    "        with open(\n",
    "            parent_dir / \"results\" / \"selected_model_names_biclfs.pkl\", \"wb\"\n",
    "        ) as f:\n",
    "            pkl.dump(selected_model_names, f)\n",
    "        with open(parent_dir / \"results\" / \"table_level_results_biclfs.pkl\", \"wb\") as f:\n",
    "            pkl.dump(table_level_results, f)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ict",
   "language": "python",
   "name": "ict"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
